#!/usr/bin/env python3

import argparse
import logging
import os
import time

import grpc
from fabric import Connection, ThreadingGroup

logging.basicConfig()
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)


def start_etcd(c, env):
    logger.info("Starting ETCD")

    env["DATACLAY_KV_HOST"] = c.host
    env["DATACLAY_KV_PORT"] = os.getenv("DATACLAY_KV_PORT", 2379)

    service_folder = env["DATACLAY_JOB_PATH"] + "/etcd"
    os.makedirs(service_folder)

    with c.cd(service_folder):
        c.run(
            f"etcd --data-dir metadata.etcd --advertise-client-urls 'http://0.0.0.0:{env['DATACLAY_KV_PORT']}' --listen-client-urls 'http://0.0.0.0:{env['DATACLAY_KV_PORT']}' &> $DATACLAY_LOG_PATH/etcd.out",
            env=env,
            asynchronous=True,
        )


def start_redis(c, env):
    logger.info("Starting Redis")

    env["DATACLAY_KV_HOST"] = c.host
    env["DATACLAY_KV_PORT"] = os.getenv("DATACLAY_KV_PORT", 6379)

    service_folder = env["DATACLAY_JOB_PATH"] + "/redis"
    os.makedirs(service_folder)

    with c.cd(service_folder):
        c.run(
            f"",
            env=env,
            asynchronous=True,
        )


def start_metadata_service(c, env):
    logger.info("Starting MetadataService")

    env["DATACLAY_METADATA_HOST"] = c.host
    env["DATACLAY_METADATA_PORT"] = os.getenv("DATACLAY_METADATA_PORT", 16587)

    # NOTE: MetadataService should be deployed on the first node.
    # check that job environ is correct.
    assert env["DATACLAY_METADATA_HOST"] == os.environ["DATACLAY_METADATA_HOST"]

    service_folder = env["DATACLAY_JOB_PATH"] + "/metadata_service"
    os.makedirs(service_folder)

    prefix = ""
    if env["DATACLAY_TRACING"]:
        prefix = "opentelemetry-instrument"
        env["OTEL_SERVICE_NAME"] = "metadata-service"

    with c.cd(service_folder):
        c.run(
            f"{prefix} python -m dataclay.metadata &> $DATACLAY_LOG_PATH/mds.out",
            env=env,
            asynchronous=True,
        )


def start_backend(c, env, num):
    logger.info(f"Starting Pyclay {num}")

    service_name = f"backend{num}"
    service_folder = f"{env['DATACLAY_JOB_PATH']}/{service_name}"
    os.makedirs(service_folder)

    env["DATACLAY_STORAGE_PATH"] = service_folder + "/storage"  # do we need?

    prefix = ""
    if env["DATACLAY_TRACING"]:
        prefix = "opentelemetry-instrument"
        env["OTEL_SERVICE_NAME"] = "pyclay"

    with c.cd(service_folder):
        c.run(
            f"{prefix} python -u -m dataclay.backend &> $DATACLAY_LOG_PATH/{service_name}.out",
            env=env,
            asynchronous=True,
        )


def deploy_dataclay(args):
    logger.info("Deploying dataClay")

    # NOTE: The first node should deploy MDS
    first_node = Connection(args.hosts[0], inline_ssh_env=True)
    other_nodes = ThreadingGroup(*args.hosts[1:], inline_ssh_env=True)

    # Dictionary to keep all environment variables
    env = {}

    # Module load
    env["PYTHONPATH"] = os.environ["PYTHONPATH"]
    env["PATH"] = os.environ["PATH"]
    env["LD_LIBRARY_PATH"] = os.environ["LD_LIBRARY_PATH"]
    env["DATACLAY_HOME"] = os.environ["DATACLAY_HOME"]

    # Dataclay vars
    env["DATACLAY_JOB_PATH"] = os.path.expandvars("$HOME/.dataclay/$SLURM_JOB_ID")
    env["DATACLAY_LOG_PATH"] = env["DATACLAY_JOB_PATH"] + "/logs"
    env["DATACLAY_USERNAME"] = os.getenv("DATACLAY_USERNAME", "testuser")
    env["DATACLAY_PASSWORD"] = os.getenv("DATACLAY_PASSWORD", "s3cret")

    # NOTE: If the file system is not shared, each service should "makedirs" the paths
    os.makedirs(env["DATACLAY_JOB_PATH"], exist_ok=True)
    os.makedirs(env["DATACLAY_LOG_PATH"], exist_ok=True)

    # Tracing using Opentelemetry
    env["DATACLAY_TRACING"] = os.getenv("DATACLAY_TRACING", "false") == "true"
    if env["DATACLAY_TRACING"]:
        with first_node.cd(os.environ["PWD"]):
            logger.info(f"Starting Opentelemetry Colector")

            # NOTE: Opentelemetry Colector should always be deployed on the first node (like MDS)
            env["OTEL_EXPORTER_OTLP_ENDPOINT"] = f"http://{first_node.host}:4317"
            assert env["OTEL_EXPORTER_OTLP_ENDPOINT"] == os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]

            env["OTEL_TRACES_SAMPLER"] = os.getenv("OTEL_TRACES_SAMPLER", "traceidratio")
            env["OTEL_TRACES_SAMPLER_ARG"] = os.getenv("OTEL_TRACES_SAMPLER_ARG", "0.1")
            # env["OTEL_EXPORTER_OTLP_INSECURE"] = "true"
            first_node.run(
                "otelcontribcol_linux_amd64 --config $DATACLAY_HOME/config/otel-json-exporter.yaml &> $DATACLAY_LOG_PATH/otel.out",
                env=env,
                asynchronous=True,
            )

    # Starting KV
    start_etcd(first_node, env)

    # Starting MetadataService
    start_metadata_service(first_node, env)

    # Wait for MetadataService to be available (#TODO: This wait should be done inside)
    logger.info(f"Waiting for MetadataService")
    grpc.channel_ready_future(
        grpc.insecure_channel(f"{env['DATACLAY_METADATA_HOST']}:{env['DATACLAY_METADATA_PORT']}")
    ).result(timeout=15)

    # Starting pyclay for each extra node
    for idx, connection in enumerate(other_nodes):
        start_backend(connection, env, idx + 1)


def stop_dataclay(args):
    logger.info("Stopping dataClay")

    first_node = Connection(args.hosts[0])
    other_nodes = ThreadingGroup(*args.hosts[1:])

    logger.info("Stopping python backends")
    other_nodes.run("killall -u $USER python")

    time.sleep(10)

    # NOTE: killall python doesn't kill "dcdeploy" because it is called as python3
    # this is very prone to errors.
    # TODO: Add to each service a gRPC call for stopping.
    logger.info("Stopping MDS")
    first_node.run("killall -u $USER python")
    logger.info("Stopping Redis")
    first_node.run("killall -u $USER redis")


def run_app(args):
    logger.info("Running application")
    nodes = ThreadingGroup(*args.hosts, inline_ssh_env=True)

    # Dictionary to keep all environment variables
    env = {}

    # Module load
    env["PYTHONPATH"] = os.environ["PYTHONPATH"]
    env["PATH"] = os.environ["PATH"]
    env["LD_LIBRARY_PATH"] = os.environ["LD_LIBRARY_PATH"]
    env["DATACLAY_HOME"] = os.environ["DATACLAY_HOME"]

    # Dataclay vars
    env["DATACLAY_JOB_PATH"] = os.path.expandvars("$HOME/.dataclay/$SLURM_JOB_ID")
    env["DATACLAY_LOG_PATH"] = env["DATACLAY_JOB_PATH"] + "/logs"
    env["DATACLAY_USERNAME"] = os.getenv("DATACLAY_USERNAME", "testuser")
    env["DATACLAY_PASSWORD"] = os.getenv("DATACLAY_PASSWORD", "s3cret")

    # Metadata config
    env["DATACLAY_METADATA_HOST"] = os.environ["DATACLAY_METADATA_HOST"]
    env["DATACLAY_METADATA_PORT"] = os.getenv("DATACLAY_METADATA_PORT", 16587)

    # Client config
    env["DC_USERNAME"] = os.getenv("DC_USERNAME", "testuser")
    env["DC_PASSWORD"] = os.getenv("DC_PASSWORD", "s3cret")
    env["DC_DATASET"] = os.getenv("DC_DATASET", "testuser")

    env["DATACLAY_TRACING"] = os.getenv("DATACLAY_TRACING", "false") == "true"
    tracing_prefix = ""
    if env["DATACLAY_TRACING"]:
        tracing_prefix = "opentelemetry-instrument"
        env["OTEL_EXPORTER_OTLP_ENDPOINT"] = os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]
        env["OTEL_TRACES_SAMPLER"] = os.getenv("OTEL_TRACES_SAMPLER", "traceidratio")
        env["OTEL_TRACES_SAMPLER_ARG"] = os.getenv("OTEL_TRACES_SAMPLER_ARG", "0.1")
        env["OTEL_SERVICE_NAME"] = "client"

    # TODO: Create a list of promises (ourput of async run), and join them at the end of all executions (like in matrix-demo.py)
    promises = []

    # NOTE: sshd_config usually has a MaxSessions=10 which avoids more than 10 processes.
    args.processes = min(max(args.processes, 1), 10)

    # Starting client execution (for testing purposes)
    for idx, connection in enumerate(nodes):
        for num_process in range(args.processes):
            logger.info(f"Running app in client {idx+1} - process {num_process+1}")
            service_name = f"client{idx+1}-{num_process+1}"

            with connection.cd(os.environ["PWD"]):
                promises.append(
                    connection.run(
                        f"{tracing_prefix} {args.command} &> $DATACLAY_LOG_PATH/{service_name}.out",
                        env=env,
                        asynchronous=True,
                    )
                )

    for p in promises:
        p.join()


# Top-level parser
parser = argparse.ArgumentParser(description="Deploy tool")
# TODO: Remove "dest" for new python versions
subparsers = parser.add_subparsers(dest="function", required=True)

# Parser for the "deploy" command
parser_deploy = subparsers.add_parser("dataclay")
parser_deploy.add_argument(
    "-H",
    "--hosts",
    nargs="+",
    required=True,
    help="hostnames to deploy dataclay. First hostname must be the client node.",
)
parser_deploy.set_defaults(func=deploy_dataclay)

# Parser for the "stop" command
parser_deploy = subparsers.add_parser("stop")
parser_deploy.add_argument(
    "-H",
    "--hosts",
    nargs="+",
    required=True,
    help="hostnames where dataclay is deployed. First hostname must be the client node.",
)
parser_deploy.set_defaults(func=stop_dataclay)

# Parser for the "run" command
parser_deploy = subparsers.add_parser("run")
parser_deploy.add_argument("command", help="command to run on each host")
parser_deploy.add_argument(
    "-p",
    "--processes",
    type=int,
    default=1,
    help="number of processes to execute in each host (max 10 due to sshd config)",
)
parser_deploy.add_argument(
    "-H", "--hosts", nargs="+", required=True, help="hostnames to run the command"
)
parser_deploy.set_defaults(func=run_app)

args = parser.parse_args()
args.func(args)
